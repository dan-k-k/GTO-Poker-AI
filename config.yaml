# config.yaml

training:
  num_episodes: 1000000     # *you can pause/resume training at anytime*; 
                            # good results after around 1,000,000 hands (maybe 50+ hrs)
  save_interval: 1000
  eval_interval: 500

agent:
  eta: 0.3              # Probability of using the average strategy (exploration)
                        # eta can start around 0.3 in first ~200,000 episodes, but lower it to 0.05 later on.
  learning_rate: 0.001  # ! Can be sensitive to change late in training !
  gamma: 1.0          # Discount factor for future rewards - No need to change from 1.0 (for terminal reward only)
  batch_size: 512
  update_frequency: 4
  target_update_frequency: 1000  

buffers:
  rl_buffer_capacity: 300000
  sl_buffer_capacity: 600000

# Simulation parameters for equity calculations
simulations:
  random_equity_trials: 200           # For the equity against random hands
  intelligent_equity_trials: 200      # For the intelligent equity against opponent range

# NEW SECTION
logging:
  dump_features: false                # for the hand log to debug

