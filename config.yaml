# config.yaml

training:
  num_episodes: 1000000     # *you can pause/resume training at anytime*; 
                            # good results after around 1,000,000 hands (maybe 50+ hrs)
  save_interval: 20000
  eval_interval: 20000

agent:
  eta: 0.3              # Probability of using the average strategy (exploration)
                        # eta can start around 0.3 in first ~200,000 episodes, but lower it to 0.05 later on.
  learning_rate: 0.001  # ! Can be sensitive to change late in training !
  gamma: 1.0          # Discount factor for future rewards - No need to change from 1.0 (for terminal reward only)
  batch_size: 512
  update_frequency: 4
  target_update_frequency: 1000  

  epsilon_start: 0.4          # Starting exploration rate (40%) Chance of uniformly random action (BR only)
  epsilon_end: 0.05           # Minimum exploration rate (.3%)
  epsilon_decay_steps: 400000 # Number of learning steps to reach the minimum

buffers:
  rl_buffer_capacity: 300000
  sl_buffer_capacity: 600000

# Simulation parameters for equity calculations
simulations:
  random_equity_trials: 200           # For the equity against random hands
  starting_stack: 200

logging:
  verbose: true                       # <--- ADD THIS
  dump_features: false                # for the hand log to debug

